{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Normal Estimation from Depth Image",
      "metadata": {
        "id": "9CagYlhclDR4",
        "cell_id": "8d81e6654aac4797af5b53ab0d1e7281",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "0719fb085f874294b5d87e0dd0b7c0be"
    },
    {
      "cell_type": "code",
      "source": "from copy import deepcopy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Rectangle\nfrom pydrake.all import RigidTransform, RotationMatrix, StartMeshcat\n\nfrom manipulation import running_as_notebook\nfrom manipulation.meshcat_utils import AddMeshcatTriad\nfrom manipulation.mustard_depth_camera_example import MustardExampleSystem",
      "metadata": {
        "id": "_GMCWQ1RjBoB",
        "cell_id": "a575fddfa6c041c4b2efe1aa5467179c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "195b4b91e6b34c1ab85470cb56032d83",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "# Start the visualizer.\nmeshcat = StartMeshcat()",
      "metadata": {
        "cell_id": "a3a25a104e284c63ba0ebd66466d6b91",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "7e502496918344a79f6ad148fce2d955",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Problem Description\nIn the lecture, we learned about estimating the point cloud normal vectors and surface curvations. For this exercise, you will investigate a slightly different approach. In particular, you will exploit the structure already presented in a depth image to avoid computing nearest neighbors. \n\n**These are the main steps of the exercise:**\n1. Implement the `estimate_normal_by_nearest_pixels` method.\n2. Come up with an example that breaks the `estimate_normal_by_nearest_pixels` method.\n\nRun the cell below to set up the simulation environment.",
      "metadata": {
        "id": "UxATaRk3jBoH",
        "cell_id": "f7f5a0c41d7e4d17a7ae83e8053326f5",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e65f69e54cd74397b22d8c7bd77be49e"
    },
    {
      "cell_type": "code",
      "source": "class NormalEstimation:\n    def __init__(self):\n        diagram = MustardExampleSystem()\n        context = diagram.CreateDefaultContext()\n\n        # setup\n        meshcat.SetProperty(\"/Background\", \"visible\", False)\n\n        # getting data\n        self.point_cloud = diagram.GetOutputPort(\"camera0_point_cloud\").Eval(context)\n        self.rgb_im = diagram.GetOutputPort(\"camera0_rgb_image\").Eval(context).data\n        self.depth_im_read = (\n            diagram.GetOutputPort(\"camera0_depth_image\").Eval(context).data.squeeze()\n        )\n        self.depth_im = deepcopy(self.depth_im_read)\n        self.depth_im[self.depth_im == np.inf] = 10.0\n        label_im = (\n            diagram.GetOutputPort(\"camera0_label_image\").Eval(context).data.squeeze()\n        )\n        self.mask = label_im == 1\n\n        # camera specs\n        cam0 = diagram.GetSubsystemByName(\"camera0\")\n        cam0_context = cam0.GetMyMutableContextFromRoot(context)\n        self.X_WC = cam0.body_pose_in_world_output_port().Eval(cam0_context)\n        self.cam_info = cam0.depth_camera_info()\n\n    def project_depth_to_pC(self, depth_pixel, uv=None):\n        \"\"\"\n        project depth pixels to points in camera frame\n        using pinhole camera model\n        Input:\n            depth_pixels: numpy array of (nx3) or (3,)\n        Output:\n            pC: 3D point in camera frame, numpy array of (nx3)\n        \"\"\"\n        # switch u,v due to python convention\n        v = depth_pixel[:, 0]\n        u = depth_pixel[:, 1]\n        Z = depth_pixel[:, 2]\n        # read camera intrinsics\n        cx = self.cam_info.center_x()\n        cy = self.cam_info.center_y()\n        fx = self.cam_info.focal_x()\n        fy = self.cam_info.focal_y()\n        X = (u - cx) * Z / fx\n        Y = (v - cy) * Z / fy\n        pC = np.c_[X, Y, Z]\n        return pC\n\n    def plot_scanning_window(self, u_range, v_range):\n        \"\"\"\n        visualize the scanning window\n        u_range: (u_start, u_end)\n        v_range: (v_start, v_end)\n        u, v are the 1st and 2nd axis of the image array\n        \"\"\"\n        # switch u, v range to get x, y\n        x0, x1 = v_range\n        y0, y1 = u_range\n        fig, ax = plt.subplots()\n        ax.imshow(self.depth_im)\n        ax.add_patch(Rectangle((x0, y0), x1 - x0, y1 - y0, alpha=0.5, fc=\"r\"))\n\n    def vis_normals(self, normals):\n        \"\"\" \"\"\"\n        for i in range(len(normals)):\n            name = \"normal_vec_{}\".format(i)\n            AddMeshcatTriad(meshcat, name, length=0.01, radius=0.001, X_PT=normals[i])\n\n\ndef bbox(img):\n    a = np.where(img != 0)\n    bbox = ([np.min(a[0]), np.max(a[0])], [np.min(a[1]), np.max(a[1])])\n    return bbox\n\n\nenv = NormalEstimation()\nmask = env.mask\ndepth_im = env.depth_im",
      "metadata": {
        "id": "STaJPvHmjBoH",
        "cell_id": "85d4bee2a62b419aa3472f715a92d966",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "149b4a3ec8e0411bad73e12a786124c4",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "The object of interest is the mustard bottle. Our goal in this exercise is to compute the estimate of point cloud normals of the mustard bottle from a depth image. The depth image is visualized below.",
      "metadata": {
        "id": "SevGI1izjBoN",
        "cell_id": "3a777c21bf684599b298b01e69e2e1be",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "dad9b073d59e4e05859dc4db75c08dba"
    },
    {
      "cell_type": "code",
      "source": "plt.imshow(depth_im)",
      "metadata": {
        "id": "TyACO5ThjBoO",
        "cell_id": "dbd67d1ccb1d40128e69249f7b4dc93b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "192444cd50d94ac48cff40f25baf52ab",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "The core idea of the approach is to exploit the fact that a depth image already includes spatial information among pixels. For example, for a selected pixel, the pixels that surround it are likely to be its nearest neighbors. Therefore, instead of computing nearest neighbors, we can instead use the nearest pixels in place of nearest neighbors. \n\nThe cell below provides a sequence of screenshots of the method, where a square/rectangular window moves across the depth image. All pixels in the sliding window is used to compute the normal vector of the center point of the window. In your implementation below, you will use a smaller window and a smaller step size to get better accuracy.",
      "metadata": {
        "id": "yDpgse_ojBoS",
        "cell_id": "3759b150cad64548af504593a57b7829",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "917e5e854f5546ea80be2ffbcf1a34db"
    },
    {
      "cell_type": "code",
      "source": "uv_step = 40\nv_bound, u_bound = bbox(mask)\n\nfor v in range(v_bound[0], v_bound[1], uv_step):\n    for u in range(u_bound[0], u_bound[1], uv_step):\n        center = [v, u]\n        u_length = 30\n        v_length = 30\n        if running_as_notebook:\n            env.plot_scanning_window(\n                [center[0] - v_length, center[0] + v_length + 1],\n                [center[1] - u_length, center[1] + u_length + 1],\n            )",
      "metadata": {
        "id": "WD7U95EDjBoT",
        "cell_id": "3d27f62669824b019011f8897c1b7ed9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "6a4847db488b48e49bcf14061e5fa981",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "## Mapping Depth Image to Point Cloud\n\nNote that pixel indices of a depth image is not a valid position measurement in the 3D world. Fortunately, there is a simple mapping from pixel locations to poses in the 3D world, and it is called the [pinhole camera model](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). We have helped you map all pixels of the depth image to points in the camera frame in the cell below. In case you need to gain direct access to this mapping, please refer to the `project_depth_to_pC` method in the `NormalEstimation` class.\n\nThe diagram below is found from [OpenCV documentation](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). Note that the $u$ and $v$ directions are reversed in Python due to the difference in convention.",
      "metadata": {
        "id": "o-8vPWxdjBoX",
        "cell_id": "458f27e452c24bd38a50405a2dee8cb6",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "40928e03831448879252d9a42a75d0c6"
    },
    {
      "cell_type": "markdown",
      "source": "![](https://docs.opencv.org/3.4/pinhole_camera_model.png)",
      "metadata": {
        "id": "OaXlVRcskbDf",
        "cell_id": "fcc30b2e2aee4eeea393c7dd609a721b",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "14352f6d6c094ccab13fb6fe04f3e843"
    },
    {
      "cell_type": "code",
      "source": "img_h, img_w = depth_im.shape\nv_range = np.arange(img_h)\nu_range = np.arange(img_w)\ndepth_u, depth_v = np.meshgrid(u_range, v_range)\ndepth_pnts = np.dstack([depth_v, depth_u, depth_im])\ndepth_pnts = depth_pnts.reshape([img_h * img_w, 3])\n# point poses in camera frame\npC = env.project_depth_to_pC(depth_pnts)",
      "metadata": {
        "id": "JHn2SKHDjBoY",
        "cell_id": "e73222f84e35480c86ac18c34098e084",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "58528fb518724320b36eb612fe60d8cb",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "## Computing Surface Normals by Nearest Pixels\nNow we should be able to calculate the surface normals. Recall from section 5.5.2 of the textbook, in each sliding window we can use the points to construct a data matrix (also known as a *scatter matrix*) **W** which exhibits special properties that allows us to estimate the local normals. \n\n**Problem 5.2.a** [2pts] Which eigenvector of **W** corresponds to the vector normal to the points in the sliding window? Assume we want to specify a normal frame whose z-axis corresponds to the vector normal to the points in the sliding window. How can you use the eigenvectors of **W** to create a rotation matrix representing the desired orientation of the normal frame. Justify that your answer represents a valid rotation matrix.",
      "metadata": {
        "id": "StDSt_MtjBoc",
        "cell_id": "44b2ec7f35274d83a29c79c8a87fd0bb",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "6393003e56e94e2ebf92f97321d1c860"
    },
    {
      "cell_type": "markdown",
      "source": "**Problem 5.2.b** [4pts] **Complete the implementation of the `estimate_normal_by_nearest_pixels` below.** \n\nNote that locations of sliding windows are provided to you for the ease of grading. The pose of the depth camera is `X_WC`, **it is a different depth camera from the one shown in the meshcat visualizer**. Lastly, **make sure the +z axis of the normal frame points outward, toward the depth camera (different from the one shown in the meshcat visualizer)**. It will be useful to review section 5.5.2 in the notes for computing the normal estimate. \n\nHINT: consider using *np.linalg.eigh*",
      "metadata": {
        "cell_id": "2878e1dc963147e6958fe573ab187973",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "3da2b620f5524e9d85b3f1d7c86c502b"
    },
    {
      "cell_type": "code",
      "source": "X_WC = env.X_WC\n\n\ndef estimate_normal_by_nearest_pixels(X_WC, pC, uv_step=10):\n    \"\"\"\n    compute the surface normals from the nearest pixels (by a sliding window)\n    Input:\n        X_WC: RigidTransform of the camera in world frame\n        pC: 3D points computed from the depth image in the camera frame\n        uv_step: recommended step size for the sliding window (see codes below)\n    Output:\n        normals: a list of RigidTransforms of the normal frames in world frame.\n                 The +z axis of the normal frame is the normal vector, it should\n                 points outward (towards the camera)\n    \"\"\"\n    normals = []\n    v_bound, u_bound = bbox(mask)\n    pC = pC.reshape(img_h, img_w, 3)\n    for v in range(v_bound[0], v_bound[1], uv_step):\n        for u in range(u_bound[0], u_bound[1], uv_step):\n            # center of the window at depth_im[u,v]\n            center = [v, u]\n            u_length = 3\n            v_length = 3\n            # side of the window\n            v_range = np.arange(max(v - v_length, 0), min(v + v_length + 1, img_h - 1))\n            u_range = np.arange(max(u - u_length, 0), min(u + u_length + 1, img_w - 1))\n\n            ###################\n            # fill your code here\n            ###################\n\n    return normals",
      "metadata": {
        "id": "arTPxyRMjBod",
        "cell_id": "34fdd7c11eaa49bf8d3d4771cb39582b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "342129b9e4f3441fa16b80f7ffd57b04",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "normals = estimate_normal_by_nearest_pixels(X_WC, pC)\nenv.vis_normals(normals)",
      "metadata": {
        "id": "PD117pt0jBol",
        "cell_id": "c25bb3f7d9cc4fb1bc72da2910f48e13",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8e3c4408ba9a4195baef07a6add34db5",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "## Normal Vector Estimation with Noisy Depth\n\n\nThe depth image tested in the first part of this exercise is a perfect depth image with no noise and missing values. Now imagine what will happen when noises and outliers are presented in the same depth image. \n\n**Problem 5.2.c** [2pts] **Illustrate a counter-example illustrating a case where the scanning window method cannot produce a good normal estimate.**",
      "metadata": {
        "id": "SfBjeNwmjBop",
        "cell_id": "c22ee9640904497abf2a158684171cc2",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "d539ca4281224e9399ff6bda65fba032"
    },
    {
      "cell_type": "markdown",
      "source": "## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza. \n\nFor submission of this assignment, you must do two things. \n- Download and submit the notebook `normal_estimation_depth.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n- Answer 5.2.a and 5.2.c in the written section of Gradescope as a part of your `pdf` submission. \n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [2 pts] Identify which eigenvector of the data matrix W corresponds to the surface normal vector and explain how the eigenvectors can be used to compute the transform of the normal frame expressed in the camera frame.\n- [4 pts] `estimate_normal_by_nearest_pixels` must be implemented correctly. \n- [2 pts] Provide a reasonable scenario that breaks the `estimate_normal_by_nearest_pixels` method.",
      "metadata": {
        "id": "w-z5MD-PjBoq",
        "cell_id": "1ab6c9dc1df24b6ca820f6dda056c708",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "f964df44abe041ff9e7a7bc9c5f036d9"
    },
    {
      "cell_type": "code",
      "source": "from manipulation.exercises.clutter.test_normal import TestNormal\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestNormal], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")",
      "metadata": {
        "id": "xWaeAZOwlz5-",
        "cell_id": "01f0ce860eda47e49b271eefeffebe93",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "41f794056cb04e359033ec98104b3883",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "d829fc1111f549c2903f20b3291d8db4",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "45496b558e544ff88357d90e67bca0e8",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a9f4175d-ea21-49c3-83c9-e5c3a0936239' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "bb8ab8fc236546c0b87feb9fefd7ef9d",
    "deepnote_execution_queue": []
  }
}