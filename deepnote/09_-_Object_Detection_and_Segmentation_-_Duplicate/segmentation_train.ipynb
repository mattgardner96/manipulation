{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n",
      "metadata": {
        "id": "DfPPQ6ztJhv4",
        "cell_id": "d9081ffe08664aa3b4d8a2c13aacd70c",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "2e95ba873ddc412b8558a29170fe6f0c"
    },
    {
      "cell_type": "code",
      "source": "!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\n# Download TorchVision repo to use some files from\n# references/detection\n!git clone https://github.com/pytorch/vision.git\n!cd vision && git checkout v0.3.0\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./\n\n# Imports\nimport fnmatch\nimport json\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]\n\n# drake_reserved_labels = [32765, 32764, 32766, 32767]\n\n\ndef colorize_labels(image):\n    \"\"\"Colorizes labels.\"\"\"\n    cc = mpl.colors.ColorConverter()\n    color_cycle = plt.rcParams[\"axes.prop_cycle\"]\n    colors = np.array([cc.to_rgb(c[\"color\"]) for c in color_cycle])\n    bg_color = [0, 0, 0]\n    image = np.squeeze(image)\n    background = np.zeros(image.shape[:2], dtype=bool)\n    for label in reserved_labels:\n        background |= image == int(label)\n    image[np.logical_not(background)]\n    color_image = colors[image % len(colors)]\n    color_image[background] = bg_color\n    return color_image",
      "metadata": {
        "id": "DBIoe_tHTQgV",
        "cell_id": "fc35e6efe6e7487394281567c1893d73",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "3baaa00ad3424e2486eca726a84deb86",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Download our bin-picking dataset\n\nIt's definitely possible to actually create this dataset on Colab; I've just written a version of the \"clutter_gen\" method from the last chapter that writes the images (and label images) to disk, along with some annotations.  But it takes a non-trivial amount of time to generate 10,000 images. \n",
      "metadata": {
        "id": "XwyE5A8DGtct",
        "cell_id": "eb293458f58c4181bd4fa4094ecdbe71",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "59396990c5184d2d9d36efc54f8751d3"
    },
    {
      "cell_type": "code",
      "source": "dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_data.zip .\n    !unzip -q clutter_maskrcnn_data.zip",
      "metadata": {
        "id": "_DgAgqauIET9",
        "cell_id": "58f8e97964f14d7db422d349994e0f34",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "fe7a009baee3419aacf4992e0165911d",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "If you are on colab, go ahead and use the file browser on the left (looks like a drive under the table of contents panel) to click through the .png and .json files to make sure you understand the dataset you've just created!  If you're on a local machine, just browse to the folder.",
      "metadata": {
        "id": "xA8sBvuHNNH1",
        "cell_id": "75775b28af1f4ac495512603193bc5e0",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "badfdbc18c6c4fb0a11358ee2eceed75"
    },
    {
      "cell_type": "markdown",
      "source": "# Teach pytorch how to load the dataset\n\ninto the [format expected by Mask R-CNN](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.maskrcnn_resnet50_fpn).",
      "metadata": {
        "id": "C9Ee5NV54Dmj",
        "cell_id": "644543187ddb4a8290234c7047bd595e",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "522bcb08f5974cbcb6625942539c5039"
    },
    {
      "cell_type": "code",
      "source": "class BinPickingDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.num_images = len(fnmatch.filter(os.listdir(root), \"*.png\"))\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        filename_base = os.path.join(self.root, f\"{idx:05d}\")\n\n        img = Image.open(filename_base + \".png\").convert(\"RGB\")\n        mask = np.squeeze(np.load(filename_base + \"_mask.npy\"))\n\n        with open(filename_base + \".json\", \"r\") as f:\n            instance_id_to_class_name = json.load(f)\n        labels = ycb == instance_id_to_class_name\n\n        # instances are encoded as different colors\n        obj_ids = np.asarray(list(instance_id_to_class_name.keys()))\n        count = (mask == np.int16(obj_ids)[:, None, None]).sum(axis=2).sum(axis=1)\n\n        # discard objects instances with less than 10 pixels\n        obj_ids = obj_ids[count >= 10]\n\n        labels = [ycb.index(instance_id_to_class_name[id] + \".sdf\") for id in obj_ids]\n        obj_ids = np.int16(np.asarray(obj_ids))\n\n        # split the color-encoded mask into a set of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return self.num_images",
      "metadata": {
        "id": "mTgWtixZTs3X",
        "cell_id": "6e364278c2e148909592b1f63a68d191",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e133ff52d04d417785fbfaaac0fae915",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's check the output of our dataset.",
      "metadata": {
        "id": "J6f3ZOTJ4Km9",
        "cell_id": "72c68f044153445c81ebc513d033e524",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "a022159850a943f5885fcb1cdae99e63"
    },
    {
      "cell_type": "code",
      "source": "dataset = BinPickingDataset(dataset_path)\ndataset[0][0]",
      "metadata": {
        "id": "ZEARO4B_ye0s",
        "cell_id": "918efe554f9346bdaeccd04bd0037ff5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "30f44ac73ad245f1afb4adcab63bd138",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Define the network\n\nThis cell is where the magic begins to happen.  We load a network that is pre-trained on the COCO dataset, then replace the network head with a new (untrained) network with the right number of outputs for our YCB recognition/segmentation task.",
      "metadata": {
        "id": "xWA2NXwVhV_C",
        "cell_id": "efdaf714e2514838aca618a109612b56",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e954815e904d4645bf17d6bfa4022b19"
    },
    {
      "cell_type": "code",
      "source": "import torchvision\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model",
      "metadata": {
        "id": "YjNHjVMOyYlH",
        "cell_id": "a4ad175c7a0d4ab6a69149c486e3fad4",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "665a8a4824bb448a866de097b1b292b3",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n\n# Transforms\n\nLet's write some helper functions for data augmentation / transformation, which leverages the functions in torchvision `refereces/detection`. \n",
      "metadata": {
        "id": "-WXLwePV5ieP",
        "cell_id": "adc805e150fd4b3d858c49d12d78852e",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "48fdd50ab2f242fa9c8a375bd5fc3459"
    },
    {
      "cell_type": "code",
      "source": "import transforms as T\nimport utils\nfrom engine import evaluate, train_one_epoch\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)",
      "metadata": {
        "id": "l79ivkwKy357",
        "cell_id": "c45aba1d054c4e0ba2d0f0b0f4091b9a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e46ca3125dee4e7eadd4b9c8ba31b2b7",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.",
      "metadata": {
        "id": "FzCLqiZk-sjf",
        "cell_id": "63ee10b21d8144049bea0823eab60627",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "dfe2e8912fd746ea8b56c81c4bbd2e22"
    },
    {
      "cell_type": "markdown",
      "source": "# Putting everything together\n\nWe now have the dataset class, the models and the data transforms. Let's instantiate them",
      "metadata": {
        "id": "3YFJGJxk6XEs",
        "cell_id": "d510dbbe897b4fe0b1aaa8c3d10ad8e9",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "748c4843bd6b41dbbeb6371008ed5bcb"
    },
    {
      "cell_type": "code",
      "source": "# use our dataset and defined transformations\ndataset = BinPickingDataset(dataset_path, get_transform(train=True))\ndataset_test = BinPickingDataset(dataset_path, get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)",
      "metadata": {
        "id": "a5dGaIezze3y",
        "cell_id": "89c9f606ae9c4b34b8ecb451eedf93a0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "0b294080dfc641ab8e2ff754b51df4ce",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Now let's instantiate the model and the optimizer",
      "metadata": {
        "id": "L5yvZUprj4ZN",
        "cell_id": "059ca9f442a042378499bc34ca79a7ad",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "56ddff3342314e61a31c9baae1886c3c"
    },
    {
      "cell_type": "code",
      "source": "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nnum_classes = len(ycb) + 1\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)",
      "metadata": {
        "id": "zoenkCj18C4h",
        "cell_id": "0d35881115ff482baefeda194b78d61f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "1f9db6618f57408b8d8c77a40162431e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "And now let's train the model for 10 epochs, evaluating at the end of every epoch.",
      "metadata": {
        "id": "XAd56lt4kDxc",
        "cell_id": "f64c11671f2b44a4aebe799ff4977477",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "fd07edb26311407a91e17bd75ed57bd7"
    },
    {
      "cell_type": "code",
      "source": "# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)",
      "metadata": {
        "id": "at-h4OWK0aoc",
        "cell_id": "291f29291a634b86aa3b7d676231b819",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "56eab5be30cd4f4fb9b51b5aa7dcfaab",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "If you're going to leave this running for a bit, I recommend scheduling the following cell to run immediately (so that you don't lose your work).",
      "metadata": {
        "id": "XXTyZhCScUTI",
        "cell_id": "32e06e5d7e1a442798ced9e74a61d0cb",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e8166cbe08374cc68b53a4174a4ecab2"
    },
    {
      "cell_type": "code",
      "source": "torch.save(model.state_dict(), \"clutter_maskrcnn_model.pt\")\n\nfrom google.colab import files\n\nfiles.download(\"clutter_maskrcnn_model.pt\")",
      "metadata": {
        "id": "vUJXn15pGzRj",
        "cell_id": "88ee04f05fb34f4b93529506373dc359",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "b407a38bcf6040ebb2e4f65018695e2b",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Now that training has finished, let's have a look at what it actually predicts in a test image",
      "metadata": {
        "id": "Z6mYGFLxkO8F",
        "cell_id": "5b799a283fea47e9a9be74bfe3d05ac5",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "4530c63ead6f4a499ea6947efc716335"
    },
    {
      "cell_type": "code",
      "source": "# pick one image from the test set\nimg, _ = dataset_test[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])",
      "metadata": {
        "id": "YHwIdxH76uPj",
        "cell_id": "9b04b4c9273847c2894c73b08f3b2c92",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "da7321d29b474a3a9819fe3b072857d4",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\nThe dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields.",
      "metadata": {
        "id": "DmN602iKsuey",
        "cell_id": "296a17a3351649db9e1823256cd8bb55",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "d05b62115c9f4273aaae47c98d1f011e"
    },
    {
      "cell_type": "code",
      "source": "prediction",
      "metadata": {
        "id": "Lkmb3qUu6zw3",
        "cell_id": "bc0a0763e952475e9f07a091f6e215cb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "5c7c23c5618f493e808981c3f429d10e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.",
      "metadata": {
        "id": "RwT21rzotFbH",
        "cell_id": "a15281a3b19740689096a86e81831915",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "97c52b85b5c940a0b4b46f48f9cd9854"
    },
    {
      "cell_type": "code",
      "source": "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())",
      "metadata": {
        "id": "bpqN9t1u7B2J",
        "cell_id": "348dfc6845c94fe296f36284603a31e9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "7d1972fee08c4dcb8e300dc1bfc7cc0a",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.",
      "metadata": {
        "id": "M58J3O9OtT1G",
        "cell_id": "48060b379f2043c68e7957d83a4acfab",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "ca8c1cc527ab47a28de25df25dd5b0f0"
    },
    {
      "cell_type": "code",
      "source": "Image.fromarray(prediction[0][\"masks\"][0, 0].mul(255).byte().cpu().numpy())",
      "metadata": {
        "id": "5v5S3bm07SO1",
        "cell_id": "be20a2803fcf4247a35615afc8bbffde",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "25b4cdb45fca43a5afa86162c0bba169",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fa95c711-a535-43df-ba6d-bc239893f168' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "fb9f426d544a4d709ae33b0f1f741d7f"
  }
}