{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n",
      "metadata": {
        "id": "DfPPQ6ztJhv4",
        "cell_id": "b42b361185464dafbe1c09d1a5ca99eb",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "aab29dd64d1d44868420a82d2e87cc9e"
    },
    {
      "cell_type": "code",
      "source": "# Imports\nimport fnmatch\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]",
      "metadata": {
        "id": "DBIoe_tHTQgV",
        "cell_id": "721e32561fdf4d5db2d9cb52a89bf8c3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e94905b5f90645fd9b835e9cea660af7",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Download our bin-picking model\n\nAnd a small set of images for testing.",
      "metadata": {
        "id": "XwyE5A8DGtct",
        "cell_id": "72a1937249964aca8d57833706967e1f",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e349fe4dd9be4d3f99fd01f274f97c19"
    },
    {
      "cell_type": "code",
      "source": "dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\n    !unzip -q clutter_maskrcnn_test.zip\n\nnum_images = len(fnmatch.filter(os.listdir(dataset_path), \"*.png\"))\n\n\ndef open_image(idx):\n    filename = os.path.join(dataset_path, f\"{idx:05d}.png\")\n    return Image.open(filename).convert(\"RGB\")\n\n\nmodel_file = \"clutter_maskrcnn_model.pt\"\nif not os.path.exists(model_file):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt .",
      "metadata": {
        "id": "_DgAgqauIET9",
        "cell_id": "3ec06494b1814b389ae9d10a8661301b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "7899ff43efdb4ef2a90c1a333907d203",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Load the model",
      "metadata": {
        "id": "xA8sBvuHNNH1",
        "cell_id": "d7769c62049c4e008465c4c02e1c48fd",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "bfca6851c67d427f9384e8e6f4b5e576"
    },
    {
      "cell_type": "code",
      "source": "import torchvision\nimport torchvision.transforms.functional as Tf\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model\n\n\nnum_classes = len(ycb) + 1\nmodel = get_instance_segmentation_model(num_classes)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\nmodel.eval()\n\nmodel.to(device)",
      "metadata": {
        "id": "vUJXn15pGzRj",
        "cell_id": "3d0be3cb6a6148d4b7e0d7adb8db20d3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "855ca78f0dd24c2d9bc1b8181dda8944",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Evaluate the network",
      "metadata": {
        "id": "Z6mYGFLxkO8F",
        "cell_id": "34340e6ae3374ccea0f1cae6c881fe2f",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "4ef0211306ee4a989571a2a3ea56fe46"
    },
    {
      "cell_type": "code",
      "source": "# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9952)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])",
      "metadata": {
        "id": "YHwIdxH76uPj",
        "cell_id": "b5e3ca651451480faaa944451311167c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8b0203f43c4b4907b5431d51d1564125",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Printing the prediction shows that we have a list of dictionaries. Each element\nof the list corresponds to a different image; since we have a single image,\nthere is a single dictionary in the list. The dictionary contains the\npredictions for the image we passed. In this case, we can see that it contains\n`boxes`, `labels`, `masks` and `scores` as fields.",
      "metadata": {
        "id": "DmN602iKsuey",
        "cell_id": "3b0b37b8dd804f99b0399c59781a9638",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "45cbe0b7eef2455aa6037ed7d014a986"
    },
    {
      "cell_type": "code",
      "source": "prediction",
      "metadata": {
        "id": "Lkmb3qUu6zw3",
        "cell_id": "960e201221ad4a55b991b31134896d5f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e50523249a2543b591a1e918b78be5af",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.",
      "metadata": {
        "id": "RwT21rzotFbH",
        "cell_id": "eebc3a84f5b2454993a832ef3e3263a6",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "0d4eb5f6c11a4534977d313a8291f245"
    },
    {
      "cell_type": "code",
      "source": "img",
      "metadata": {
        "id": "bpqN9t1u7B2J",
        "cell_id": "87cb6a70f9c7413184d73b5c7fe76d4a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "bfcc19c7f74d43949aca976d057cd7d8",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.",
      "metadata": {
        "id": "M58J3O9OtT1G",
        "cell_id": "2186ee4297cf40c4a98d4d070d12f5b8",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "820aa16c86f64679a796b5f4d4c51e87"
    },
    {
      "cell_type": "code",
      "source": "N = prediction[0][\"masks\"].shape[0]\nfig, ax = plt.subplots(N, 1, figsize=(15, 15))\nfor n in range(prediction[0][\"masks\"].shape[0]):\n    ax[n].imshow(\n        np.asarray(\n            Image.fromarray(prediction[0][\"masks\"][n, 0].mul(255).byte().cpu().numpy())\n        )\n    )",
      "metadata": {
        "id": "5v5S3bm07SO1",
        "cell_id": "cdecdedfb5e54c128559c9c992809b94",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "511a74d0ff114201b94328fb2280dc0c",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Plot the object detections",
      "metadata": {
        "id": "z9QAeX9HkDTx",
        "cell_id": "4cf34a1e41ac478793e7408617234580",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "f8d31163018b4426aa8fcce9dc7e2636"
    },
    {
      "cell_type": "code",
      "source": "import random\n\nimport matplotlib.patches as patches\n\n\ndef plot_prediction():\n    img_np = np.array(img)\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(img_np)\n\n    cmap = plt.get_cmap(\"tab20b\")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    num_instances = prediction[0][\"boxes\"].shape[0]\n    bbox_colors = random.sample(colors, num_instances)\n    boxes = prediction[0][\"boxes\"].cpu().numpy()\n    labels = prediction[0][\"labels\"].cpu().numpy()\n\n    for i in range(num_instances):\n        color = bbox_colors[i]\n        bb = boxes[i, :]\n        bbox = patches.Rectangle(\n            (bb[0], bb[1]),\n            bb[2] - bb[0],\n            bb[3] - bb[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor=\"none\",\n        )\n        ax.add_patch(bbox)\n        plt.text(\n            bb[0],\n            bb[0],\n            s=ycb[labels[i]],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": color, \"pad\": 0},\n        )\n    plt.axis(\"off\")\n\n\nplot_prediction()",
      "metadata": {
        "id": "Z08keVFkvtPh",
        "cell_id": "1575c5a3053f493dad6c012b6f6071d6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "9b1cb9e425a84089b2b51d01a9f35d1e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Visualize the region proposals \n\nLet's visualize some of the intermediate results of the networks.\n\nTODO: would be very cool to put a slider on this so that we could slide through ALL of the boxes.  But my matplotlib non-interactive backend makes it too tricky!",
      "metadata": {
        "id": "HIfmykN-t7XG",
        "cell_id": "0d2fae6fad5a42f0b4d5c749755365eb",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "1fa187951e204ad7b291870b1d5bc534"
    },
    {
      "cell_type": "code",
      "source": "class Inspector:\n    \"\"\"A helper class from Kuni to be used for torch.nn.Module.register_forward_hook.\"\"\"\n\n    def __init__(self):\n        self.x = None\n\n    def hook(self, module, input, output):\n        self.x = output\n\n\ninspector = Inspector()\nmodel.rpn.register_forward_hook(inspector.hook)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nrpn_values = inspector.x\n\n\nimg_np = np.array(img)\nplt.figure()\nfig, ax = plt.subplots(1, figsize=(12, 9))\nax.imshow(img_np)\n\ncmap = plt.get_cmap(\"tab20b\")\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\nnum_to_draw = 20\nbbox_colors = random.sample(colors, num_to_draw)\nboxes = rpn_values[0][0].cpu().numpy()\nprint(f\"Region proposals (drawing first {num_to_draw} out of {boxes.shape[0]})\")\n\nfor i in range(num_to_draw):\n    color = bbox_colors[i]\n    bb = boxes[i, :]\n    bbox = patches.Rectangle(\n        (bb[0], bb[1]),\n        bb[2] - bb[0],\n        bb[3] - bb[1],\n        linewidth=2,\n        edgecolor=color,\n        facecolor=\"none\",\n    )\n    ax.add_patch(bbox)\nplt.axis(\"off\");",
      "metadata": {
        "id": "zBNqFb68td8N",
        "cell_id": "f6cc16708db340bd9fa94aeea39abe02",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8dd39ba830634ec985807b11726fbce5",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Try a few more images",
      "metadata": {
        "cell_id": "92b171d68d374b75b8dc49af5f77f455",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "10ba315b5ace491ca8a02f950b90e7f2"
    },
    {
      "cell_type": "code",
      "source": "# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9985)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nplot_prediction()",
      "metadata": {
        "cell_id": "817287b8d79748058a703cc9c9f92f2b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "38ebeae735af4e5fb5fdfabf284398bc",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "981ff5f234354ca5b8082bcea796b8e8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "270e6ef2413d420ebde1bb55e0a860ee",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fa95c711-a535-43df-ba6d-bc239893f168' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "7a47588f39fa414bb9085bed998f75d6"
  }
}