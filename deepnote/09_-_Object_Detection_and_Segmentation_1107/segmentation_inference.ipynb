{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n",
      "metadata": {
        "id": "DfPPQ6ztJhv4",
        "cell_id": "7b3f4797d6e94ffba905f0ff073870d9",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "aab29dd64d1d44868420a82d2e87cc9e"
    },
    {
      "cell_type": "code",
      "source": "# Imports\nimport fnmatch\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]",
      "metadata": {
        "id": "DBIoe_tHTQgV",
        "cell_id": "2544170a065341fe9e25574fa286578d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e94905b5f90645fd9b835e9cea660af7",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Download our bin-picking model\n\nAnd a small set of images for testing.",
      "metadata": {
        "id": "XwyE5A8DGtct",
        "cell_id": "45e6eaefe9f2477ca6e9b8a1f153901a",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e349fe4dd9be4d3f99fd01f274f97c19"
    },
    {
      "cell_type": "code",
      "source": "dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\n    !unzip -q clutter_maskrcnn_test.zip\n\nnum_images = len(fnmatch.filter(os.listdir(dataset_path), \"*.png\"))\n\n\ndef open_image(idx):\n    filename = os.path.join(dataset_path, f\"{idx:05d}.png\")\n    return Image.open(filename).convert(\"RGB\")\n\n\nmodel_file = \"clutter_maskrcnn_model.pt\"\nif not os.path.exists(model_file):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt .",
      "metadata": {
        "id": "_DgAgqauIET9",
        "cell_id": "0545af65e753440ba020d55cc5fd2b0d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "7899ff43efdb4ef2a90c1a333907d203",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Load the model",
      "metadata": {
        "id": "xA8sBvuHNNH1",
        "cell_id": "fd3a4787d43c429086c6187eba2ec8b7",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "bfca6851c67d427f9384e8e6f4b5e576"
    },
    {
      "cell_type": "code",
      "source": "import torchvision\nimport torchvision.transforms.functional as Tf\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model\n\n\nnum_classes = len(ycb) + 1\nmodel = get_instance_segmentation_model(num_classes)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\nmodel.eval()\n\nmodel.to(device)",
      "metadata": {
        "id": "vUJXn15pGzRj",
        "cell_id": "d93f96a3f32a4772957f38dd99cd1e86",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "855ca78f0dd24c2d9bc1b8181dda8944",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Evaluate the network",
      "metadata": {
        "id": "Z6mYGFLxkO8F",
        "cell_id": "7ecdf2591bc44e2ab45ae4d2040e1f88",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "4ef0211306ee4a989571a2a3ea56fe46"
    },
    {
      "cell_type": "code",
      "source": "# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9952)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])",
      "metadata": {
        "id": "YHwIdxH76uPj",
        "cell_id": "e6b55bf2f5ff4ec9a366df1e3b42d1d7",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8b0203f43c4b4907b5431d51d1564125",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Printing the prediction shows that we have a list of dictionaries. Each element\nof the list corresponds to a different image; since we have a single image,\nthere is a single dictionary in the list. The dictionary contains the\npredictions for the image we passed. In this case, we can see that it contains\n`boxes`, `labels`, `masks` and `scores` as fields.",
      "metadata": {
        "id": "DmN602iKsuey",
        "cell_id": "23b44b28b03443a281c900a8b5b197d8",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "45cbe0b7eef2455aa6037ed7d014a986"
    },
    {
      "cell_type": "code",
      "source": "prediction",
      "metadata": {
        "id": "Lkmb3qUu6zw3",
        "cell_id": "985fef9cf34a43538b6ce5c79b0d7db9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "e50523249a2543b591a1e918b78be5af",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.",
      "metadata": {
        "id": "RwT21rzotFbH",
        "cell_id": "b507d388fce44d0f984219dae08ff169",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "0d4eb5f6c11a4534977d313a8291f245"
    },
    {
      "cell_type": "code",
      "source": "img",
      "metadata": {
        "id": "bpqN9t1u7B2J",
        "cell_id": "7d2bbb2b27fd47fd949455c895299c75",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "bfcc19c7f74d43949aca976d057cd7d8",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.",
      "metadata": {
        "id": "M58J3O9OtT1G",
        "cell_id": "0729591152794a69b6115e93dfa8096d",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "820aa16c86f64679a796b5f4d4c51e87"
    },
    {
      "cell_type": "code",
      "source": "N = prediction[0][\"masks\"].shape[0]\nfig, ax = plt.subplots(N, 1, figsize=(15, 15))\nfor n in range(prediction[0][\"masks\"].shape[0]):\n    ax[n].imshow(\n        np.asarray(\n            Image.fromarray(prediction[0][\"masks\"][n, 0].mul(255).byte().cpu().numpy())\n        )\n    )",
      "metadata": {
        "id": "5v5S3bm07SO1",
        "cell_id": "fe9fc2d112d84c398bc605573e319804",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "511a74d0ff114201b94328fb2280dc0c",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Plot the object detections",
      "metadata": {
        "id": "z9QAeX9HkDTx",
        "cell_id": "5ba32275f1224590b33bfa0badcb0699",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "f8d31163018b4426aa8fcce9dc7e2636"
    },
    {
      "cell_type": "code",
      "source": "import random\n\nimport matplotlib.patches as patches\n\n\ndef plot_prediction():\n    img_np = np.array(img)\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(img_np)\n\n    cmap = plt.get_cmap(\"tab20b\")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    num_instances = prediction[0][\"boxes\"].shape[0]\n    bbox_colors = random.sample(colors, num_instances)\n    boxes = prediction[0][\"boxes\"].cpu().numpy()\n    labels = prediction[0][\"labels\"].cpu().numpy()\n\n    for i in range(num_instances):\n        color = bbox_colors[i]\n        bb = boxes[i, :]\n        bbox = patches.Rectangle(\n            (bb[0], bb[1]),\n            bb[2] - bb[0],\n            bb[3] - bb[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor=\"none\",\n        )\n        ax.add_patch(bbox)\n        plt.text(\n            bb[0],\n            bb[0],\n            s=ycb[labels[i]],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": color, \"pad\": 0},\n        )\n    plt.axis(\"off\")\n\n\nplot_prediction()",
      "metadata": {
        "id": "Z08keVFkvtPh",
        "cell_id": "1d074a5b3d6c4c3c8d01802f7c8a83c0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "9b1cb9e425a84089b2b51d01a9f35d1e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Visualize the region proposals \n\nLet's visualize some of the intermediate results of the networks.\n\nTODO: would be very cool to put a slider on this so that we could slide through ALL of the boxes.  But my matplotlib non-interactive backend makes it too tricky!",
      "metadata": {
        "id": "HIfmykN-t7XG",
        "cell_id": "94d22af4f2d94cce94f41634f4f210b5",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "1fa187951e204ad7b291870b1d5bc534"
    },
    {
      "cell_type": "code",
      "source": "class Inspector:\n    \"\"\"A helper class from Kuni to be used for torch.nn.Module.register_forward_hook.\"\"\"\n\n    def __init__(self):\n        self.x = None\n\n    def hook(self, module, input, output):\n        self.x = output\n\n\ninspector = Inspector()\nmodel.rpn.register_forward_hook(inspector.hook)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nrpn_values = inspector.x\n\n\nimg_np = np.array(img)\nplt.figure()\nfig, ax = plt.subplots(1, figsize=(12, 9))\nax.imshow(img_np)\n\ncmap = plt.get_cmap(\"tab20b\")\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\nnum_to_draw = 20\nbbox_colors = random.sample(colors, num_to_draw)\nboxes = rpn_values[0][0].cpu().numpy()\nprint(f\"Region proposals (drawing first {num_to_draw} out of {boxes.shape[0]})\")\n\nfor i in range(num_to_draw):\n    color = bbox_colors[i]\n    bb = boxes[i, :]\n    bbox = patches.Rectangle(\n        (bb[0], bb[1]),\n        bb[2] - bb[0],\n        bb[3] - bb[1],\n        linewidth=2,\n        edgecolor=color,\n        facecolor=\"none\",\n    )\n    ax.add_patch(bbox)\nplt.axis(\"off\");",
      "metadata": {
        "id": "zBNqFb68td8N",
        "cell_id": "8aaf50b7a49b4fbfa043cef5ec8fe023",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8dd39ba830634ec985807b11726fbce5",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Try a few more images",
      "metadata": {
        "cell_id": "ef7cb0b0e98a4cf0b4600aaad0f01e1c",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "10ba315b5ace491ca8a02f950b90e7f2"
    },
    {
      "cell_type": "code",
      "source": "# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9985)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nplot_prediction()",
      "metadata": {
        "cell_id": "e9f5dfc4426c4984b107bd65b6095353",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "38ebeae735af4e5fb5fdfabf284398bc",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "67ef20e07cef4a059f8a7f6a82e03d33",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "270e6ef2413d420ebde1bb55e0a860ee",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7d9a4e0e-79b9-4dd3-8018-83b670a6abf2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "ddab898ea15747e3968cf23fb1358ea4"
  }
}