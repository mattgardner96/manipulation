{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Contrastive Loss in Dense Object Nets",
      "metadata": {
        "collapsed": false,
        "cell_id": "067c8e9782524a2f8d97711817f28e24",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "b1a7ca2e1a0449ccba0ba6a6a1c01865"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "collapsed": false,
        "cell_id": "59aba1bd8732406fb98f15a34580bf42",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "ee83d41c9221495ba8b4fd9807c20e7b",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "## Intro\nIn this notebook, you will be working on implementing the loss for [dense object net](https://arxiv.org/abs/1806.08756).\n\nWhen doing robotic manipulation with perception, we sometimes need to find pixel correspondences between two images. For example, we have an image of a mug that we've pre-computed optimal contact position on the handle. We know the pixel that correspond to the contact position in this source image. Now, given an image of a cluttered scene containing the mug, we want to identify the pixel that correspond to the same contact position on handle in order to transfer the grasp to the new scene.\n\n<p style=\"text-align:center;\"><img src=\"https://iili.io/tSulS4.png\" width=\"500\"></p>\n\nIn reality, we don't want to train one correspondence model for every single possible interested point on the object. Instead, we hope to train one model that can help us establish correspondence for arbitary points on the object, such as handle, joint, a point on lid etc.\n\nTo achieve this, we use convolution neural network to parameterize a dense descriptor $f$. Given an image $I$ of shape HxWx3, $f$ takes in $I$ and outputs a D-dimensional feature vector at every pixel. That is, $f(I) \\in \\mathbb{R}^{H\\times W\\times D}$. We can now specify an index $(i, j)$ in 2d image frame, the feature vector $f(I)[i,j]$ 'describes' the sematic meaning of pixel $I[i,j]$. Intuitively, if the feature at two pixel locations are similiar, we should get a similiar descriptor vector. Notice $f$ is a fully convolutional neural network, so $f(I)[i,j]$ depends on not just pixel $I[i,j]$ but also its neighborhood.\n\nWe now offer a (loose) mathematical formulation of 'describes':\nGiven two images, $I_a, I_b$ and a pair of coordinates, $(u_a, v_a)$ and $(u_b, v_b)$, if the pixel $I_a[i_a, j_a]$ and $I_b[i_b, j_b]$ correspond to the same point on the object (e.g tip of a pencil), we hope $|| f(I_a)[i_a, j_a] - f(I_b)[i_b, j_b]||_2$ is as small as possible. If they correspond to distinct points on the object (e.g. pencil tip and rubber), $|| f(I_a)[i_a, j_a] - f(I_b)[i_b, j_b]||_2$ should be as big as possible.\n\nIf we project the D-dimensional features at each pixel location to 3-dim RGB space, we can see correponding pairs will share the same color.\n<p style=\"text-align:center;\"><img src=\"https://iili.io/tSzhiJ.md.png\" width=\"600\"></p>",
      "metadata": {
        "collapsed": false,
        "cell_id": "b9d7c09e5fdb493d9082a77d922767ed",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "325997e9ba7446839a023fb101b56b5b"
    },
    {
      "cell_type": "markdown",
      "source": "## a) Loss Implementation\nIn the code block below, you are asked to implement the loss in Dense Object Net (DON). Read section 3.1 of the [paper](https://arxiv.org/pdf/1806.08756.pdf) and fill out the following function. In DON paper, $u$ instead of $(i, j)$ is used as the notation for index in camera plane. The variable $u$ in the code block uses the definition from the DON paper.\n\nBy deep learning convention, you are not allowed to use any kind of loops in the function to make your code fast to compute.\n\n\nNotes: \n\n-In deep learning, the data always comes in batches. So a batch of image indices $u$ will have shape (N, 2), where N is the batch size. Usually images come in batches too, but in DON we are sampling a large batch size of indices for every image, therefore input images are not batched here. f(), however still expects batched input, so an image batch size of 1 will do.\n\n-After obtaining $f(I_a)$ and $f(I_b)$, you will need to calculate the loss over the batch of image indices $u_a, u_b$\n\n-If you haven't already, now is a good time to become familiar with numpy array indexing and slicing.\n\n",
      "metadata": {
        "collapsed": false,
        "cell_id": "b05231333cfc46d9a9c1dba53b77f767",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "e817b5de685d4adebf634f10912084dc"
    },
    {
      "cell_type": "code",
      "source": "def don_loss(f, img_a, img_b, u_a, u_b, match, margin=2.0):\n    \"\"\"\n    Compute DON loss with a batch of data\n    Args:\n        f: a neural network that takes in a batch of images with 3 channels and outputs dense features with D channels for each pixel location. e.g. f(I) has shape (N, H, W, D) for I of shape (N, H, W, 3)\n        img_a: np.ndarray with shape (H, W, 3), an image\n        img_b: np.ndarray with shape (H, W, 3), an image\n        u_a: np.ndarray with shape (N, 2), a batch of indices (row_idx, col_idx) to index location in img_a\n        u_b: np.ndarray with shape (N, 2), a batch of indices (row_idx, col_idx) to index location in img_b\n        match: np.ndarray with shape (N, 1), a batch of boolean variables that indicates match or not\n        margin: the margin parameter M in DON paper section 3.1\n    Return:\n        loss_matches: a float whose value is the L_matches in DON paper section 3.1\n        loss_nonmatches: a float whose value is the L_non-matches in DON paper section 3.1\n    \"\"\"\n    ### Your code here ###\n    # Note you are not allowed to use loops! Instead, use google to find needed numpy functions\n    loss_matches = 0.0  # modify me\n    loss_nonmatches = 0.0  # modify me\n\n    return loss_matches, loss_nonmatches",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "collapsed": false,
        "cell_id": "d89f0643715842feb66f6ac2f1e93d3d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8afef562c525497b92b1288dd69a4c7f",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "Since we are not training DON, we used numpy instead of commonly used deep learning frameworks like pytorch or jax. These frameworks have built in auto-differentiation and shares very similiar grammar with numpy. With the loss implemented, all we need to do is keep sampling data, and call auto-differentiation to perform gradient descent to train the network.",
      "metadata": {
        "collapsed": false,
        "cell_id": "29bd6ec46ad746898d23ece64de36bab",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "83a2671c49aa441f992f248d1e996f7e"
    },
    {
      "cell_type": "markdown",
      "source": "## b) Prediction\nAfter DON is trained, given image $I_a$ and a pixel location $u_a$, we can use $f$ to find a corresponding location $u_b$ in $I_b$ such that the features at these two locations are the closest. Implement the following function to do inference for DON.",
      "metadata": {
        "collapsed": false,
        "cell_id": "a2fb9142125f4bcd8654f052349ec248",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "20aa984aa22c4debb302fca88d9b22b1"
    },
    {
      "cell_type": "code",
      "source": "def don_predict(f, img_a, img_b, u_a):\n    \"\"\"\n    Run trained DON to find correponding point coordinate in image b.\n    Args:\n        f: a neural network that takes in a batch of images with 3 channels and outputs dense features with D channels for each pixel location. e.g. f(I) has shape (N, H, W, D) for I of shape (N, H, W, 3)\n        img_a: np.ndarray with shape (H, W, 3), an image\n        img_b: np.ndarray with shape (H, W, 3), an image\n        u_a: np.ndarray with shape (2, ), row and col indices in img_a that specifies the point on object\n    Return:\n        u_b: np.ndarray with shape (2, ), row and col indices in img_b that correspond to the point on object\n    \"\"\"\n    ### Your code here\n    u_b = np.zeros(2)  # modify me\n    return u_b",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "collapsed": false,
        "cell_id": "2b3baff2338840c5a00d891864b277ff",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "02046ad5cc1c4f94bb13c18b2055f89e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza.\n\nFor submission of this assignment, you must do two things.\n- Download and submit the notebook `constrastive.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [4 pts] `don_loss` must be implemented correctly.\n- [4 pts] `don_predict` must be implemented correctly.",
      "metadata": {
        "collapsed": false,
        "cell_id": "11193c9321164dd6a7da8f163269b69a",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "a964fc558f134036a24ab00ce7db5bbf"
    },
    {
      "cell_type": "code",
      "source": "from manipulation.exercises.deep_perception.test_contrastive import TestContrastive\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestContrastive], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")",
      "metadata": {
        "collapsed": false,
        "cell_id": "db09028a81644977b9624fc9b10b77d8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "6d86a3689b4447dbb355948c02649747",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3ce5dece-684b-4bfd-90bf-19d2696cb2f7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "6a80b264e7cf43e688cd6b1922caa1ff"
  }
}